{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Boundary Detection for MIDI Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import mido\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakh_midi_dir = Path(\"../data/lmd_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "torch.random.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "TARGET_TICKS_PER_BEAT = 4\n",
    "WINDOW_HALF_TICKS = 256\n",
    "INSTRUMENT_OVERTONES = True\n",
    "NUM_TARGETS = 1 # Additional targets within 2**i bars of center where i < NUM_TARGETS\n",
    "SEPARATE_DRUMS = True\n",
    "PATCH_NORMALIZE = True\n",
    "PRETRAIN = True\n",
    "POSITIVE_OVERSAMPLING_FACTOR = 2\n",
    "NEGATIVE_UNDERSAMPLING_FACTOR = 1\n",
    "PAD_PIANO_ROLL = False # For handling boundary patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKERS = json.load(open(\"../markers_qn.json\"))\n",
    "\n",
    "def parse_midi(file_path):\n",
    "    \"\"\"\n",
    "    Parse a MIDI file into a list of bar segments per track.\n",
    "    A bar segment is defined as a list of MIDI messages encoded as tuples that fit into a single bar.\n",
    "    A tuple is defined as (time, note, velocity, duration, channel, program)\n",
    "    \"\"\"\n",
    "    midi = mido.MidiFile(file_path, clip=True)\n",
    "\n",
    "    track_data = {\n",
    "        (track.name if track.name else f\"track_{idx}\"): []\n",
    "        for idx, track in enumerate(midi.tracks)\n",
    "    }\n",
    "\n",
    "    file_name = os.path.basename(file_path).split('.mid')[0]\n",
    "    marker_qns = MARKERS[file_name]\n",
    "    markers_ticks = [int(round(x[0] * midi.ticks_per_beat)) for x in marker_qns]\n",
    "\n",
    "    channel_volumes = {\n",
    "        i: 127\n",
    "        for i in range(16)\n",
    "    }\n",
    "    channel_expressions = {\n",
    "        i: 127\n",
    "        for i in range(16)\n",
    "    }\n",
    "    channel_instruments = {\n",
    "        i: 0\n",
    "        for i in range(16)\n",
    "    }\n",
    "\n",
    "    for idx, track in enumerate(midi.tracks):\n",
    "        track_name = track.name if track.name else f\"track_{idx}\"\n",
    "        current_ticks = 0\n",
    "        for msg in track:\n",
    "            current_ticks += msg.time\n",
    "            if msg.type == \"control_change\":\n",
    "                if msg.control == 7:\n",
    "                    channel_volumes[msg.channel] = msg.value\n",
    "                elif msg.control == 11:\n",
    "                    channel_expressions[msg.channel] = msg.value\n",
    "            elif msg.type == \"program_change\":\n",
    "                channel_instruments[msg.channel] = msg.program\n",
    "            elif msg.type == \"marker\":\n",
    "                pass\n",
    "            elif msg.type == \"note_on\" and msg.velocity > 0:\n",
    "                velocity = msg.velocity * (channel_volumes[msg.channel] / 127.) * (\n",
    "                            channel_expressions[msg.channel] / 127.)\n",
    "                program = channel_instruments[msg.channel]\n",
    "                track_data[track_name].append({\n",
    "                    \"time\": current_ticks,\n",
    "                    \"note\": msg.note,\n",
    "                    \"velocity\": velocity,\n",
    "                    \"duration\": -1,\n",
    "                    \"channel\": msg.channel,\n",
    "                    \"program\": program\n",
    "                })\n",
    "            elif msg.type == \"note_off\" or (msg.type == \"note_on\" and msg.velocity == 0):\n",
    "                for note in track_data[track_name]:\n",
    "                    if note[\"duration\"] == -1 and note[\"note\"] == msg.note and note[\"channel\"] == msg.channel:\n",
    "                        note[\"duration\"] = current_ticks - note[\"time\"]\n",
    "                        break\n",
    "\n",
    "    # Remove duplicate marker ticks\n",
    "    markers_ticks = list(set(markers_ticks))\n",
    "    markers_ticks.sort()\n",
    "\n",
    "    return track_data, markers_ticks, midi.ticks_per_beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instrument_overtone_intensities(program, num_harmonics=3, max_harmonic=5):\n",
    "    \"\"\"\n",
    "    Generate a set of harmonics and their intensities for a given instrument program.\n",
    "    The harmonics are random but fixed for a given program.\n",
    "    \"\"\"\n",
    "    np.random.seed(hash(str(program)) % 2**32)\n",
    "\n",
    "    harmonics = np.sort(np.random.choice(max_harmonic, num_harmonics, replace=False) + 2)\n",
    "    intensities = np.sort(np.random.rand(num_harmonics))[::-1]\n",
    "\n",
    "    # Return to original seed\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    return harmonics, intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument_categories = {\n",
    "    \"Piano\": range(0, 8), \"Chromatic Percussion\": range(8, 16),\n",
    "    \"Organ\": range(16, 24), \"Guitar\": range(24, 32),\n",
    "    \"Bass\": range(32, 40), \"Strings\": range(40, 48),\n",
    "    \"Ensemble\": range(48, 56), \"Brass\": range(56, 64),\n",
    "    \"Reed\": range(64, 72), \"Pipe\": range(72, 80),\n",
    "    \"Synth Lead\": range(80, 88), \"Synth Pad\": range(88, 96),\n",
    "    \"Synth Effects\": range(96, 104), \"Ethnic\": range(104, 112),\n",
    "    \"Percussive\": range(112, 120), \"Sound Effects\": range(120, 128)\n",
    "}\n",
    "\n",
    "[instrument_overtone_intensities(instr) for instr in instrument_categories.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hz_to_midi(frequency):\n",
    "    if frequency <= 0:\n",
    "        raise ValueError(\"Frequency must be greater than 0 Hz.\")\n",
    "    return 69 + 12 * np.log2(frequency / 440.0)\n",
    "\n",
    "def midi_to_hz(midi_note):\n",
    "    return 440.0 * 2**((midi_note - 69) / 12)\n",
    "\n",
    "hz_to_midi(440), midi_to_hz(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_piano_roll(\n",
    "    note_data,\n",
    "    ticks_per_beat,\n",
    "    chroma=False,\n",
    "    target_ticks_per_beat=24,\n",
    "    instrument_overtones=False,\n",
    "    separate_drums=False\n",
    "):\n",
    "    if len(note_data) == 0:\n",
    "        return None\n",
    "    num_notes = 12 if chroma else 128\n",
    "    duration_ticks = note_data[-1][\"time\"] + note_data[-1][\"duration\"]\n",
    "    piano_roll = np.zeros((3, num_notes, duration_ticks))\n",
    "\n",
    "    for note in note_data:\n",
    "        # fixed duration for drum tracks since we only need the onsets\n",
    "        drum_track = note[\"channel\"] == 9\n",
    "        duration = 1 if drum_track else note[\"duration\"]\n",
    "\n",
    "        start = note[\"time\"]\n",
    "        end = min(start + duration, duration_ticks)\n",
    "        if end - start <= 0:\n",
    "            continue\n",
    "\n",
    "        pitch_class = note[\"note\"] % 12 if chroma else note[\"note\"]\n",
    "\n",
    "        velocity = note[\"velocity\"]\n",
    "        piano_roll_channel = 2 if drum_track and separate_drums else 0\n",
    "        piano_roll[piano_roll_channel, pitch_class, start:end] = velocity\n",
    "        if not instrument_overtones:\n",
    "            piano_roll[1, pitch_class, start:end] = velocity\n",
    "\n",
    "        if drum_track and not separate_drums:\n",
    "            piano_roll[0, pitch_class, start:end] = velocity\n",
    "\n",
    "        # Add overtones\n",
    "        if instrument_overtones and not drum_track:\n",
    "            program = note[\"program\"]\n",
    "            harmonics, intensities = instrument_overtone_intensities(program)\n",
    "            pitch = midi_to_hz(note[\"note\"])\n",
    "            max_intensity = intensities[0]\n",
    "            for harmonic, intensity in zip(harmonics, intensities):\n",
    "                overtone_pitch = pitch * harmonic\n",
    "                overtone_midi = hz_to_midi(overtone_pitch)\n",
    "                overtone_pitch_class = overtone_midi % 12 if chroma else overtone_midi\n",
    "                overtone_pitch_class = int(np.round(overtone_pitch_class))\n",
    "                if overtone_pitch_class <= 127:\n",
    "                    decay = np.linspace(1.0, 0.0, end - start) * intensity / max_intensity\n",
    "                    piano_roll[1, overtone_pitch_class, start:end] = velocity * intensity * decay\n",
    "\n",
    "    # Downsample to target_ticks_per_beat ticks per beat using max pooling\n",
    "    if ticks_per_beat > target_ticks_per_beat:\n",
    "        pool_size = ticks_per_beat // target_ticks_per_beat\n",
    "        try:\n",
    "            piano_roll = F.max_pool1d(torch.tensor(piano_roll), pool_size, stride=pool_size).numpy()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(piano_roll.shape)\n",
    "            return None\n",
    "    \n",
    "\n",
    "    return piano_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_piano_roll(piano_roll, start_tick, num_ticks, label=None, markers_ticks=None):\n",
    "    if markers_ticks is None:\n",
    "        markers_ticks = []\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # 3 plots, one for the main notes, one for the overtones, and one for the drums\n",
    "\n",
    "    x_min = start_tick\n",
    "    x_max = start_tick + num_ticks\n",
    "    y_min = 0\n",
    "    y_max = piano_roll.shape[-2]\n",
    "    extent = (x_min, x_max, y_min, y_max)\n",
    "\n",
    "    plt.subplot(3, 1, 1)\n",
    "    for marker in markers_ticks:\n",
    "        if marker >= x_min and marker <= x_max:\n",
    "            plt.axvline(marker, color=\"red\", linestyle=\"--\")\n",
    "    plt.imshow(piano_roll[0, :, start_tick:start_tick + num_ticks], aspect=\"auto\", origin=\"lower\", cmap=\"viridis\", extent=extent)\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.imshow(piano_roll[1, :, start_tick:start_tick + num_ticks], aspect=\"auto\", origin=\"lower\", cmap=\"viridis\", extent=extent)\n",
    "\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.imshow(piano_roll[2, :, start_tick:start_tick + num_ticks], aspect=\"auto\", origin=\"lower\", cmap=\"viridis\", extent=extent)\n",
    "\n",
    "    plt.xlabel(\"Time (ticks)\")\n",
    "    plt.ylabel(\"Note\")\n",
    "    plt.title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_take(one_in_n: int) -> bool:\n",
    "    return (torch.randint(0, one_in_n, ()) < 1).bool().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakh_test_example = \"0148c9c216484115f87daac532ef57db\"\n",
    "\n",
    "midi_path = lakh_midi_dir / Path(f\"{lakh_test_example[0]}\") / Path(f\"{lakh_test_example}.mid\")\n",
    "midi = mido.MidiFile(midi_path)\n",
    "\n",
    "track_data, markers_ticks, _ = parse_midi(midi_path)\n",
    "\n",
    "track_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Piano Roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piano_rolls = []\n",
    "# drum_piano_roll = None  # Separate channel\n",
    "for track_name, note_data in track_data.items():\n",
    "    piano_roll = create_piano_roll(\n",
    "        note_data,\n",
    "        midi.ticks_per_beat,\n",
    "        chroma=False,\n",
    "        target_ticks_per_beat=TARGET_TICKS_PER_BEAT,\n",
    "        instrument_overtones=INSTRUMENT_OVERTONES,\n",
    "        separate_drums=SEPARATE_DRUMS\n",
    "    )\n",
    "    # Some tracks are empty\n",
    "    if piano_roll is None:\n",
    "        continue\n",
    "    piano_rolls.append(piano_roll)\n",
    "\n",
    "actual_length = max(piano_roll.shape[-1] for piano_roll in piano_rolls)\n",
    "for i, piano_roll in enumerate(piano_rolls):\n",
    "    piano_rolls[i] = torch.nn.functional.pad(torch.tensor(piano_roll), (0, actual_length - piano_roll.shape[-1]))\n",
    "\n",
    "print([piano_roll.shape for piano_roll in piano_rolls])\n",
    "piano_roll = torch.stack(piano_rolls)\n",
    "# Merge channels\n",
    "piano_roll = piano_roll.sum(dim=0).clamp(0, 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piano_roll.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Piano Roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if midi.ticks_per_beat > TARGET_TICKS_PER_BEAT:\n",
    "    downsample_factor = midi.ticks_per_beat // TARGET_TICKS_PER_BEAT\n",
    "    markers_ticks_downsampled = [marker // downsample_factor for marker in markers_ticks]\n",
    "\n",
    "track_name = \"track_4\"\n",
    "drum_track = \"drum\" in track_name.lower()   # TODO: look for channel 9 instead\n",
    "piano_roll_bass = create_piano_roll(track_data[track_name], midi.ticks_per_beat, chroma=False, instrument_overtones=True)\n",
    "\n",
    "plot_piano_roll(piano_roll_bass, 9233 - WINDOW_HALF_TICKS, WINDOW_HALF_TICKS * 2, label=track_name, markers_ticks=markers_ticks_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precompute piano rolls and save features and labels in a structured manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_files = json.load(open(\"../data/good_files_3_7_2025.json\"))\n",
    "\n",
    "DATA_DIR = Path(f\"/Volumes/ExtremePro/lakh_data_{TARGET_TICKS_PER_BEAT}_overtones_{INSTRUMENT_OVERTONES}_separate_drums_{SEPARATE_DRUMS}\")\n",
    "def create_lakh_dataset():\n",
    "    \"\"\"\n",
    "    Loads MIDI files from the Lakh MIDI dataset, processes them into piano rolls,\n",
    "    and saves them in a structured directory format for training, validation, and testing.\n",
    "    The dataset is split into training, validation, and test sets based on the provided good files.\n",
    "    The processed data is saved in PyTorch tensor format.\n",
    "    The directory structure is as follows:\n",
    "    - DATA_DIR/\n",
    "        - tubb_train/\n",
    "        - non_tubb_train/\n",
    "        - tubb_val/\n",
    "        - non_tubb_val/\n",
    "        - tubb_test/\n",
    "        - non_tubb_test/\n",
    "    \"\"\"\n",
    "    if not DATA_DIR.exists():\n",
    "        DATA_DIR.mkdir()\n",
    "\n",
    "    Path(DATA_DIR / \"tubb_train\").mkdir(exist_ok=True)\n",
    "    Path(DATA_DIR / \"non_tubb_train\").mkdir(exist_ok=True)\n",
    "    Path(DATA_DIR / \"tubb_val\").mkdir(exist_ok=True)\n",
    "    Path(DATA_DIR / \"non_tubb_val\").mkdir(exist_ok=True)\n",
    "    Path(DATA_DIR / \"tubb_test\").mkdir(exist_ok=True)\n",
    "    Path(DATA_DIR / \"non_tubb_test\").mkdir(exist_ok=True)\n",
    "\n",
    "    measure_qns_all = json.load(open(\"../data/measures_qn.json\"))\n",
    "    for key in good_files:\n",
    "        print(f\"Processing files: {key}\")\n",
    "        for test_example in tqdm(good_files[key], desc=\"Loading test examples\"):\n",
    "            save_path = DATA_DIR / Path(key) / Path(f\"{test_example}.pt\")\n",
    "            if save_path.exists():\n",
    "                continue\n",
    "\n",
    "            measure_qns = measure_qns_all[test_example]\n",
    "            midi_path = lakh_midi_dir / Path(f\"{test_example[0]}\") / Path(test_example + \".mid\")\n",
    "            if not midi_path.exists():\n",
    "                print(f\"Missing MIDI file: {midi_path}\")\n",
    "                continue\n",
    "\n",
    "            # MIDI\n",
    "            try:\n",
    "                track_data, markers_ticks, ticks_per_beat = parse_midi(midi_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading MIDI file: {midi_path}\")\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            # Annotation\n",
    "            if ticks_per_beat > TARGET_TICKS_PER_BEAT:\n",
    "                markers_ticks = [int(round(marker * TARGET_TICKS_PER_BEAT / ticks_per_beat)) for marker in markers_ticks]\n",
    "                measure_ticks = [int(round(qn * TARGET_TICKS_PER_BEAT)) for qn in measure_qns]\n",
    "            else:\n",
    "                print(f\"Skipping {test_example} due to downsample factor\")\n",
    "                continue\n",
    "\n",
    "            piano_rolls = []\n",
    "            # drum_piano_roll = None  # Separate channel\n",
    "            for track_name, note_data in track_data.items():\n",
    "                piano_roll = create_piano_roll(\n",
    "                    note_data,\n",
    "                    ticks_per_beat,\n",
    "                    chroma=False,\n",
    "                    target_ticks_per_beat=TARGET_TICKS_PER_BEAT,\n",
    "                    instrument_overtones=INSTRUMENT_OVERTONES,\n",
    "                    separate_drums=SEPARATE_DRUMS\n",
    "                )\n",
    "                # Some tracks are empty\n",
    "                if piano_roll is None:\n",
    "                    continue\n",
    "                piano_rolls.append(piano_roll)\n",
    "\n",
    "            if len(piano_rolls) == 0:\n",
    "                print(f\"Skipping {test_example} due to empty piano rolls\")\n",
    "                continue\n",
    "\n",
    "            actual_length = max(piano_roll.shape[-1] for piano_roll in piano_rolls)\n",
    "            for i, piano_roll in enumerate(piano_rolls):\n",
    "                piano_rolls[i] = torch.nn.functional.pad(torch.tensor(piano_roll), (0, actual_length - piano_roll.shape[-1]))\n",
    "\n",
    "            piano_roll = torch.stack(piano_rolls)\n",
    "            # Merge channels\n",
    "            piano_roll = piano_roll.sum(dim=0).clamp(0, 127)\n",
    "\n",
    "            # Additionally pad 4 bars to each side to allow for segment extraction\n",
    "            # piano_roll = torch.nn.functional.pad(piano_roll, (WINDOW_HALF_TICKS, WINDOW_HALF_TICKS))\n",
    "            # markers_ticks = [marker + WINDOW_HALF_TICKS for marker in markers_ticks]\n",
    "            # measure_ticks = [measure_tick + WINDOW_HALF_TICKS for measure_tick in measure_ticks]\n",
    "\n",
    "            torch.save({\n",
    "                \"piano_roll\": piano_roll.to(torch.float32),\n",
    "                \"segment_boundaries\": torch.tensor(markers_ticks).to(torch.float32),\n",
    "                \"measure_ticks\": torch.tensor(measure_ticks).to(torch.float32)\n",
    "            }, save_path)\n",
    "            \n",
    "create_lakh_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and process (padding, etc.) precomputed piano rolls and prepare the patch metadata DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths defined above in create_lakh_dataset()\n",
    "piano_roll_paths = [path for path in (DATA_DIR / \"tubb_train\").iterdir() if path.suffix == \".pt\" and not path.name.startswith(\".\")] + \\\n",
    "                   [path for path in (DATA_DIR / \"non_tubb_train\").iterdir() if path.suffix == \".pt\" and not path.name.startswith(\".\")] + \\\n",
    "                   [path for path in (DATA_DIR / \"tubb_val\").iterdir() if path.suffix == \".pt\" and not path.name.startswith(\".\")] + \\\n",
    "                   [path for path in (DATA_DIR / \"non_tubb_val\").iterdir() if path.suffix == \".pt\" and not path.name.startswith(\".\")] + \\\n",
    "                     [path for path in (DATA_DIR / \"tubb_test\").iterdir() if path.suffix == \".pt\" and not path.name.startswith(\".\")] + \\\n",
    "                     [path for path in (DATA_DIR / \"non_tubb_test\").iterdir() if path.suffix == \".pt\" and not path.name.startswith(\".\")]\n",
    "\n",
    "def get_piano_rolls():\n",
    "    \"\"\"\n",
    "    Load piano rolls from the specified paths, process them, and return a list of piano rolls\n",
    "    and a dictionary of patch data.\n",
    "    Handles positive oversampling and negative undersampling and pads the piano rolls if specified.\n",
    "    \"\"\"\n",
    "    padding = WINDOW_HALF_TICKS\n",
    "\n",
    "    piano_rolls = []\n",
    "    patch_data = {}\n",
    "    sample_idx = 0\n",
    "    piano_roll_idx = 0\n",
    "\n",
    "    for piano_roll_path in tqdm(piano_roll_paths, desc=\"Loading inputs and labels\"):\n",
    "        try:\n",
    "            data = torch.load(piano_roll_path)\n",
    "        except RuntimeError:\n",
    "            print(f\"Error loading {piano_roll_path}\")\n",
    "            continue\n",
    "\n",
    "        positive_oversampling_factor = POSITIVE_OVERSAMPLING_FACTOR if 'train' in str(piano_roll_path) else 1\n",
    "        negative_undersampling_factor = NEGATIVE_UNDERSAMPLING_FACTOR if 'train' in str(piano_roll_path) else 1\n",
    "\n",
    "        piano_roll = data[\"piano_roll\"]\n",
    "        segment_boundaries = data[\"segment_boundaries\"]\n",
    "        measure_boundaries = data[\"measure_ticks\"]\n",
    "\n",
    "        # Compute first and last nonzero columns of the first channel (first and last onset, respectively)\n",
    "        if piano_roll.dim() == 4:\n",
    "            batch_mask = piano_roll[0]  # Select the first batch \n",
    "        else:\n",
    "            batch_mask = piano_roll\n",
    "        channel_mask = batch_mask[0]  # Select the first channel\n",
    "\n",
    "        # Find nonzero column indices\n",
    "        nonzero_indices = channel_mask.nonzero(as_tuple=True)\n",
    "        if nonzero_indices[1].numel() > 0:\n",
    "            first_nonzero_column = nonzero_indices[1].min().item()\n",
    "            last_nonzero_column = nonzero_indices[1].max().item()\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Throw out markers before first onset or after last onset\n",
    "        segment_boundaries = segment_boundaries[segment_boundaries > first_nonzero_column]\n",
    "        segment_boundaries = segment_boundaries[segment_boundaries < last_nonzero_column]\n",
    "        measure_boundaries = measure_boundaries[measure_boundaries > first_nonzero_column]\n",
    "        measure_boundaries = measure_boundaries[measure_boundaries < last_nonzero_column]\n",
    "\n",
    "        # Add first and last nonzero column to the segment boundaries\n",
    "        segment_boundaries = torch.cat([\n",
    "            torch.tensor([first_nonzero_column], dtype=torch.float32, device=piano_roll.device),\n",
    "            segment_boundaries,\n",
    "            torch.tensor([last_nonzero_column], dtype=torch.float32, device=piano_roll.device)\n",
    "        ])\n",
    "        measure_boundaries = torch.cat([\n",
    "            torch.tensor([first_nonzero_column], dtype=torch.float32, device=piano_roll.device),\n",
    "            measure_boundaries,\n",
    "            torch.tensor([last_nonzero_column], dtype=torch.float32, device=piano_roll.device)\n",
    "        ])\n",
    "\n",
    "        # Crop piano roll to the first and last onset\n",
    "        piano_roll = piano_roll[..., first_nonzero_column:last_nonzero_column + 1]\n",
    "        # Adjust segment boundaries to the cropped piano roll\n",
    "        segment_boundaries -= first_nonzero_column\n",
    "        measure_boundaries -= first_nonzero_column\n",
    "\n",
    "        # Pad piano roll to the left and right for boundary segment extraction\n",
    "        if PAD_PIANO_ROLL:\n",
    "            piano_roll = F.pad(piano_roll, (padding, padding), mode='constant', value=0)\n",
    "            segment_boundaries += padding\n",
    "            measure_boundaries += padding\n",
    "\n",
    "        piano_rolls.append(piano_roll)\n",
    "\n",
    "        for i in measure_boundaries:\n",
    "            if not PAD_PIANO_ROLL and (i - padding <= 0 or i + padding >= piano_roll.shape[-1]):\n",
    "                continue\n",
    "\n",
    "            is_segment_boundary = (segment_boundaries == i).any().item()\n",
    "            repetitions = positive_oversampling_factor if is_segment_boundary == 1. else int(random_take(one_in_n=negative_undersampling_factor))\n",
    "\n",
    "            nearest_segment_boundary = segment_boundaries[torch.argmin(torch.abs(segment_boundaries - i))].item()\n",
    "\n",
    "            sample = {\n",
    "                # Metadata\n",
    "                \"filename\": piano_roll_path.stem,\n",
    "                \"from\": i - padding,\n",
    "                \"to\": i + padding,\n",
    "                # Data\n",
    "                \"piano_roll_idx\": piano_roll_idx,\n",
    "                \"patch_idx\": i,\n",
    "                \"is_segment_boundary\": is_segment_boundary,\n",
    "                \"key\": piano_roll_path.parent.stem, # non_tubb_train, non_tubb_val, tubb_train, tubb_val\n",
    "\n",
    "                # New: nearest segment boundary\n",
    "                \"nearest_segment_boundary\": nearest_segment_boundary\n",
    "            }\n",
    "\n",
    "            for _ in range(repetitions):\n",
    "                patch_data[sample_idx] = sample\n",
    "                sample_idx += 1\n",
    "\n",
    "        piano_roll_idx += 1\n",
    "    return piano_rolls, patch_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piano_rolls, patch_data = get_piano_rolls()\n",
    "metadata_df = pd.DataFrame.from_dict(patch_data, orient=\"index\")\n",
    "\n",
    "num_piano_rolls = len(piano_rolls)\n",
    "num_patches = metadata_df.shape[0]\n",
    "\n",
    "train_mask = metadata_df[\"key\"].str.contains(\"train\")\n",
    "val_mask = metadata_df[\"key\"].str.contains(\"val\")\n",
    "test_mask = metadata_df[\"key\"].str.contains(\"test\")\n",
    "\n",
    "segment_boundary_mask = metadata_df[\"is_segment_boundary\"] == True\n",
    "\n",
    "num_train_patches = metadata_df[train_mask].shape[0]\n",
    "num_positive_train_patches = metadata_df[train_mask & segment_boundary_mask].shape[0]\n",
    "num_negative_train_patches = metadata_df[train_mask & ~segment_boundary_mask].shape[0]\n",
    "\n",
    "num_val_patches = metadata_df[val_mask].shape[0]\n",
    "num_positive_val_patches = metadata_df[val_mask & segment_boundary_mask].shape[0]\n",
    "num_negative_val_patches = metadata_df[val_mask & ~segment_boundary_mask].shape[0]\n",
    "\n",
    "num_test_patches = metadata_df[test_mask].shape[0]\n",
    "num_positive_test_patches = metadata_df[test_mask & segment_boundary_mask].shape[0]\n",
    "num_negative_test_patches = metadata_df[test_mask & ~segment_boundary_mask].shape[0]\n",
    "\n",
    "print(f\"Total number of piano rolls: {num_piano_rolls}\")\n",
    "print(f\"Total number of patches: {num_patches}\")\n",
    "print(f\"Number of train patches: {num_train_patches}\")\n",
    "print(f\"Number of positive train patches: {num_positive_train_patches}\")\n",
    "print(f\"Number of negative train patches: {num_negative_train_patches}\")\n",
    "print(f\"Number of val patches: {num_val_patches}\")\n",
    "print(f\"Number of positive val patches: {num_positive_val_patches}\")\n",
    "print(f\"Number of negative val patches: {num_negative_val_patches}\")\n",
    "print(f\"Number of test patches: {num_test_patches}\")\n",
    "print(f\"Number of positive test patches: {num_positive_test_patches}\")\n",
    "print(f\"Number of negative test patches: {num_negative_test_patches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch PianoRollDataset Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_augmentation(piano_roll, transpose_range=6):\n",
    "    transpose_amount = torch.randint(-transpose_range, transpose_range, ())\n",
    "    return torch.roll(piano_roll, transpose_amount.item(), dims=-2)\n",
    "\n",
    "\n",
    "class PianoRollDataset(Dataset):\n",
    "    def __init__(self, piano_rolls, metadata_df, normalize=False, transpose_augmentation=True):\n",
    "        self.piano_rolls = piano_rolls\n",
    "        self.metadata_df = metadata_df\n",
    "        self.normalize = normalize\n",
    "        self.transpose_augmentation = transpose_augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.metadata_df.loc[idx]\n",
    "        piano_roll = self.piano_rolls[sample[\"piano_roll_idx\"]]\n",
    "        patch = piano_roll[..., sample[\"from\"].int():sample[\"to\"].int()]\n",
    "\n",
    "        center = sample[\"from\"] + (sample[\"to\"] - sample[\"from\"]) / 2\n",
    "        nearest_segment_boundary = sample[\"nearest_segment_boundary\"]\n",
    "\n",
    "        # targets: boundary at center? boundary within (2, 4, 8) bars of center?\n",
    "        # TODO: assumes a single segment boundary per patch\n",
    "        main_target = [sample[\"is_segment_boundary\"]]\n",
    "        additional_targets = [(nearest_segment_boundary - center).abs() <= 2**i * TARGET_TICKS_PER_BEAT * 4 for i in range(NUM_TARGETS - 1)]\n",
    "        \n",
    "        targets = torch.tensor(main_target + additional_targets).to(torch.float32)\n",
    "\n",
    "        if self.normalize:\n",
    "            patch = patch / patch.max() \n",
    "\n",
    "        if self.transpose_augmentation:\n",
    "            patch = transpose_augmentation(patch)\n",
    "\n",
    "        return patch, targets\n",
    "    \n",
    "    def metadata_at(self, idx):\n",
    "        sample = self.metadata_df.loc[idx]\n",
    "        return {\n",
    "            \"filename\": sample[\"filename\"],\n",
    "            \"from\": sample[\"from\"],\n",
    "            \"to\": sample[\"to\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df[\"is_segment_boundary\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Dataset Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PianoRollDataset(piano_rolls, metadata_df, normalize=True)\n",
    "\n",
    "sample_idx = int(torch.randint(0, len(dataset), ()))\n",
    "patch, targets = dataset[sample_idx]\n",
    "is_segment_boundary = targets[0]\n",
    "\n",
    "patch_info = dataset.metadata_at(sample_idx)\n",
    "from_tick, to_tick, filename = patch_info[\"from\"], patch_info[\"to\"], patch_info[\"filename\"]\n",
    "print(f\"Sample {sample_idx} from {filename} ({from_tick} to {to_tick}), is_segment_boundary: {is_segment_boundary.item()}\")\n",
    "\n",
    "plt.figure(figsize=(6.4*2, 4*2))\n",
    "\n",
    "cmap = 'gist_yarg'\n",
    "center_tick = (from_tick + to_tick) / 2\n",
    "\n",
    "# Plot channel 0\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.imshow(patch.squeeze().permute(1, 2, 0)[..., 0], aspect=\"auto\", origin=\"lower\", cmap=cmap, extent=[from_tick, to_tick, 0, patch.shape[-2]])\n",
    "plt.axvline(x=center_tick, color=\"r\", alpha=0.5)\n",
    "plt.xlabel(\"Time (ticks)\")\n",
    "plt.ylabel(\"Note\")\n",
    "plt.title(f\"Channel 0 (Piano Roll)\")\n",
    "\n",
    "# Plot channel 1\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.imshow(patch.squeeze().permute(1, 2, 0)[..., 1], aspect=\"auto\", origin=\"lower\", cmap=cmap, extent=[from_tick, to_tick, 0, patch.shape[-2]])\n",
    "plt.axvline(x=center_tick, color=\"r\", alpha=0.5)\n",
    "plt.xlabel(\"Time (ticks)\")\n",
    "plt.ylabel(\"Note\")\n",
    "plt.title(\"Channel 1 (Overtones)\")\n",
    "\n",
    "# Plot channel 2\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.imshow(patch.squeeze().permute(1, 2, 0)[..., 2], aspect=\"auto\", origin=\"lower\", cmap=cmap, extent=[from_tick, to_tick, 0, patch.shape[-2]])\n",
    "plt.axvline(x=center_tick, color=\"r\", alpha=0.5)\n",
    "plt.xlabel(\"Time (ticks)\")\n",
    "plt.ylabel(\"Note\")\n",
    "plt.title(\"Channel 2 (Drums)\")\n",
    "\n",
    "# # Plot combined\n",
    "# plt.subplot(4, 1, 4)\n",
    "# plt.imshow(patch.squeeze().permute(1, 2, 0), aspect=\"auto\", origin=\"lower\", cmap=\"gray\", extent=[patch_info[\"from\"], patch_info[\"to\"], 0, patch.shape[-2]])\n",
    "# plt.axvline(x=(patch_info[\"from\"] + patch_info[\"to\"]) / 2, color=\"r\", alpha=0.5)\n",
    "# plt.xlabel(\"Time (ticks)\")\n",
    "# plt.ylabel(\"Note\")\n",
    "# plt.title(f\"Combined (RGB representation)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "SAVE_PLOT = False\n",
    "if SAVE_PLOT:\n",
    "    plt.savefig(os.path.expanduser(f\"~/Downloads/patch_{filename}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN boundary classifier\n",
    "class BoundaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        weights = torchvision.models.MobileNet_V3_Small_Weights.DEFAULT\n",
    "        backbone = torchvision.models.mobilenet_v3_small(weights=weights)\n",
    "        backbone.classifier[-1] = nn.Sequential(\n",
    "            nn.Linear(backbone.classifier[-1].in_features, NUM_TARGETS),\n",
    "        )\n",
    "        for layer in backbone.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight.data)\n",
    "        self.backbone = backbone\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# class BoundaryClassifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=(6, 8))\n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=(6, 3))\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 6))\n",
    "#         self.dense = nn.Linear(64 * 18 * 504, 128)\n",
    "#         self.out = nn.Linear(128, NUM_TARGETS)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = self.pool1(x)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = F.relu(self.dense(x))\n",
    "#         return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_prec_recall(input, target):\n",
    "    tp_count = ((input > 0) & (target == 1)).sum().item()\n",
    "    fp_count = ((input > 0) & (target == 0)).sum().item()\n",
    "    tn_count = ((input <= 0) & (target == 0)).sum().item()\n",
    "    fn_count = ((input <= 0) & (target == 1)).sum().item()\n",
    "\n",
    "    accuracy = (tp_count + tn_count) / (tp_count + tn_count + fp_count + fn_count) if tp_count + tn_count + fp_count + fn_count > 0 else 0\n",
    "    precision = tp_count / (tp_count + fp_count) if tp_count + fp_count > 0 else 0\n",
    "    recall = tp_count / (tp_count + fn_count) if tp_count + fn_count > 0 else 0\n",
    "\n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(input, target):\n",
    "    results = {}\n",
    "    for i in range(input.size(-1)):\n",
    "        acc, prec, recall = acc_prec_recall(input[..., i], target[..., i])\n",
    "        results[\"accuracy_\" + str(i)] = acc\n",
    "        results[\"precision_\" + str(i)] = prec\n",
    "        results[\"recall_\" + str(i)] = recall\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "RESUME_TRAINING = False\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = metadata_df.sample(frac=1)\n",
    "metadata_train = metadata_df[metadata_df[\"key\"].isin([\"tubb_train\", \"non_tubb_train\"])]\n",
    "metadata_val_tubb = metadata_df[metadata_df[\"key\"] == \"tubb_val\"]\n",
    "metadata_val_non_tubb = metadata_df[metadata_df[\"key\"] == \"non_tubb_val\"]\n",
    "metadata_train.reset_index(drop=True, inplace=True)\n",
    "metadata_val_tubb.reset_index(drop=True, inplace=True)\n",
    "metadata_val_non_tubb.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dataset_train = PianoRollDataset(piano_rolls, metadata_train, normalize=PATCH_NORMALIZE)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dataset_val_tubb = PianoRollDataset(piano_rolls, metadata_val_tubb, normalize=PATCH_NORMALIZE)\n",
    "dataloader_val_tubb = DataLoader(dataset_val_tubb, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "dataset_val_non_tubb = PianoRollDataset(piano_rolls, metadata_val_non_tubb, normalize=PATCH_NORMALIZE)\n",
    "dataloader_val_non_tubb = DataLoader(dataset_val_non_tubb, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "SAVE_PATH = Path(\"./models\")\n",
    "SAVE_PATH.mkdir(exist_ok=True)\n",
    "model_name = Path(f\"pretrain_{PRETRAIN}_mn_overtones_{INSTRUMENT_OVERTONES}_normalized_{PATCH_NORMALIZE}_separate_drums_{SEPARATE_DRUMS}_targets_{NUM_TARGETS}.pt\")\n",
    "\n",
    "model = BoundaryClassifier().to(device)\n",
    "\n",
    "if RESUME_TRAINING and (model_path := SAVE_PATH / model_name).exists():\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "val_losses_tubb = []\n",
    "val_accuracies_tubb = []\n",
    "val_precisions_tubb = []\n",
    "val_recalls_tubb = []\n",
    "\n",
    "val_losses_non_tubb = []\n",
    "val_accuracies_non_tubb = []\n",
    "val_precisions_non_tubb = []\n",
    "val_recalls_non_tubb = []\n",
    "\n",
    "metrics_tubb_all = []\n",
    "metrics_non_tubb_all = []\n",
    "\n",
    "best_model = model\n",
    "best_val_f1 = 0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Reload piano rolls and reinitialize dataloaders if we have negative undersampling to reshuffle\n",
    "    if NEGATIVE_UNDERSAMPLING_FACTOR:\n",
    "        piano_rolls, patch_data = get_piano_rolls()\n",
    "        metadata_df = pd.DataFrame.from_dict(patch_data, orient=\"index\")\n",
    "\n",
    "        metadata_df = metadata_df.sample(frac=1)\n",
    "        metadata_train = metadata_df[metadata_df[\"key\"].isin([\"tubb_train\", \"non_tubb_train\"])]\n",
    "        metadata_val_tubb = metadata_df[metadata_df[\"key\"] == \"tubb_val\"]\n",
    "        metadata_val_non_tubb = metadata_df[metadata_df[\"key\"] == \"non_tubb_val\"]\n",
    "        metadata_train.reset_index(drop=True, inplace=True)\n",
    "        metadata_val_tubb.reset_index(drop=True, inplace=True)\n",
    "        metadata_val_non_tubb.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        train_fnames = set(metadata_train['filename'])\n",
    "        val_tubb_fnames = set(metadata_val_tubb['filename'])\n",
    "        val_non_tubb_fnames = set(metadata_val_non_tubb['filename'])\n",
    "\n",
    "        dataset_train = PianoRollDataset(piano_rolls, metadata_train, normalize=PATCH_NORMALIZE)\n",
    "        dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        dataset_val_tubb = PianoRollDataset(piano_rolls, metadata_val_tubb, normalize=PATCH_NORMALIZE)\n",
    "        dataloader_val_tubb = DataLoader(dataset_val_tubb, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        dataset_val_non_tubb = PianoRollDataset(piano_rolls, metadata_val_non_tubb, normalize=PATCH_NORMALIZE)\n",
    "        dataloader_val_non_tubb = DataLoader(dataset_val_non_tubb, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Log some labeled images from the training dataloader\n",
    "    example_batch = next(iter(dataloader_train))\n",
    "    example_images, example_targets = example_batch\n",
    "\n",
    "    for i in range(min(4, len(example_images))):\n",
    "        label = example_targets[i]\n",
    "        writer.add_image(\n",
    "            tag=f\"Train/Example_image_{i}_label_{label}\",\n",
    "            img_tensor=example_images[i],\n",
    "            global_step=epoch,\n",
    "            dataformats=\"CHW\",\n",
    "        )\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    step = 0\n",
    "    for batch in (pbar := tqdm(dataloader_train)):\n",
    "        pbar.set_description(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        piano_roll, targets = batch\n",
    "        piano_roll, targets = piano_roll.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(piano_roll)\n",
    "        loss = criterion(output, targets.float().to(device))\n",
    "        writer.add_scalar(\"Loss/Train\", loss, epoch * len(dataloader_train) + step)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix({\"Train Loss (current/average)\": f\"{loss.item():.4f}/{train_loss / (step + 1):.4f}\"})\n",
    "\n",
    "        step += 1\n",
    "    writer.flush()\n",
    "    train_loss /= len(dataloader_train)\n",
    "\n",
    "    model.eval()\n",
    "    val_outputs_tubb = []\n",
    "    val_targets_tubb = []\n",
    "    val_outputs_non_tubb = []\n",
    "    val_targets_non_tubb = []\n",
    "    with torch.no_grad():\n",
    "        val_loss_tubb = 0\n",
    "        val_loss_non_tubb = 0\n",
    "        for batch_tubb in dataloader_val_tubb:\n",
    "            piano_roll, targets = batch_tubb\n",
    "            piano_roll, targets = piano_roll.to(device), targets.to(device)\n",
    "\n",
    "            output = model(piano_roll)\n",
    "\n",
    "            val_outputs_tubb.append(output)\n",
    "            val_targets_tubb.append(targets)\n",
    "\n",
    "            loss = criterion(output, targets.float().to(device))\n",
    "            val_loss_tubb += loss.item()\n",
    "        for batch_non_tubb in dataloader_val_non_tubb:\n",
    "            piano_roll, targets = batch_non_tubb\n",
    "            piano_roll, targets = piano_roll.to(device), targets.to(device)\n",
    "\n",
    "            output = model(piano_roll)\n",
    "\n",
    "            val_outputs_non_tubb.append(output)\n",
    "            val_targets_non_tubb.append(targets)\n",
    "\n",
    "            loss = criterion(output, targets.float().to(device))\n",
    "            val_loss_non_tubb += loss.item()\n",
    "\n",
    "        val_loss_tubb /= len(dataloader_val_tubb)\n",
    "        val_loss_non_tubb /= len(dataloader_val_non_tubb)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    metrics_tubb = compute_metrics(torch.cat(val_outputs_tubb), torch.cat(val_targets_tubb))\n",
    "    val_losses_tubb.append(val_loss_tubb)\n",
    "    val_accuracies_tubb.append(metrics_tubb[\"accuracy_0\"])\n",
    "    val_precisions_tubb.append(metrics_tubb[\"precision_0\"])\n",
    "    val_recalls_tubb.append(metrics_tubb[\"recall_0\"])\n",
    "\n",
    "    metrics_non_tubb = compute_metrics(torch.cat(val_outputs_non_tubb), torch.cat(val_targets_non_tubb))\n",
    "    val_losses_non_tubb.append(val_loss_non_tubb)\n",
    "    val_accuracies_non_tubb.append(metrics_non_tubb[\"accuracy_0\"])\n",
    "    val_precisions_non_tubb.append(metrics_non_tubb[\"precision_0\"])\n",
    "    val_recalls_non_tubb.append(metrics_non_tubb[\"recall_0\"])\n",
    "\n",
    "    metrics_tubb_all.append(metrics_tubb)\n",
    "    metrics_non_tubb_all.append(metrics_non_tubb)\n",
    "\n",
    "    f1_tubb = 2 * (metrics_tubb[\"precision_0\"] * metrics_tubb[\"recall_0\"]) / (metrics_tubb[\"precision_0\"] + metrics_tubb[\"recall_0\"]) if metrics_tubb[\"precision_0\"] + metrics_tubb[\"recall_0\"] > 0 else 0\n",
    "    f1_non_tubb = 2 * (metrics_non_tubb[\"precision_0\"] * metrics_non_tubb[\"recall_0\"]) / (metrics_non_tubb[\"precision_0\"] + metrics_non_tubb[\"recall_0\"]) if metrics_non_tubb[\"precision_0\"] + metrics_non_tubb[\"recall_0\"] > 0 else 0\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss (Tubb/Non-Tubb): ({val_loss_tubb:.4f}, {val_loss_non_tubb:.4f}), Val F1 (Tubb/Non-Tubb): ({f1_tubb:.4f}, {f1_non_tubb:.4f})\")\n",
    "\n",
    "    val_f1 = (f1_tubb + f1_non_tubb) / 2\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_model = model\n",
    "        torch.save(model.state_dict(), SAVE_PATH / model_name)\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= 5:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # --- Log metrics to TensorBoard ---\n",
    "    writer.add_scalar(\"Loss/Val_Tubb\", val_loss_tubb, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Val_Tubb\", metrics_tubb[\"accuracy_0\"], epoch)\n",
    "    writer.add_scalar(\"Precision/Val_Tubb\", metrics_tubb[\"precision_0\"], epoch)\n",
    "    writer.add_scalar(\"Recall/Val_Tubb\", metrics_tubb[\"recall_0\"], epoch)\n",
    "\n",
    "    writer.add_scalar(\"Loss/Val_Non_Tubb\", val_loss_non_tubb, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Val_Non_Tubb\", metrics_non_tubb[\"accuracy_0\"], epoch)\n",
    "    writer.add_scalar(\"Precision/Val_Non_Tubb\", metrics_non_tubb[\"precision_0\"], epoch)\n",
    "    writer.add_scalar(\"Recall/Val_Non_Tubb\", metrics_non_tubb[\"recall_0\"], epoch)\n",
    "\n",
    "    writer.flush()\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# save metrics_tubb_all, metrics_non_tubb_all as json\n",
    "json.dump(metrics_tubb_all, open(f\"./metrics_tubb_all_overtones_{INSTRUMENT_OVERTONES}_normalized_{PATCH_NORMALIZE}_separate_drums_{SEPARATE_DRUMS}_targets_{NUM_TARGETS}_2.json\", \"w\"))\n",
    "json.dump(metrics_non_tubb_all, open(f\"./metrics_non_tubb_all_overtones_{INSTRUMENT_OVERTONES}_normalized_{PATCH_NORMALIZE}_separate_drums_{SEPARATE_DRUMS}_targets_{NUM_TARGETS}_2.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * (np.mean(val_precisions_tubb) * np.mean(val_recalls_tubb)) / (np.mean(val_precisions_tubb) + np.mean(val_recalls_tubb)), 2 * (np.mean(val_precisions_non_tubb) * np.mean(val_recalls_non_tubb)) / (np.mean(val_precisions_non_tubb) + np.mean(val_recalls_non_tubb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), SAVE_PATH / model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracies_non_tubb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "ax[0].plot(train_losses, label=\"Train Loss\")\n",
    "ax[0].plot(val_losses_tubb[::2], label=\"Val Loss\")\n",
    "ax[0].set_title(\"Loss\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(val_accuracies_tubb[::2], label=\"Accuracy\")\n",
    "ax[1].set_title(\"Accuracy\")\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(val_precisions_tubb[::2], label=\"Precision\")\n",
    "ax[2].set_title(\"Precision\")\n",
    "ax[2].legend()\n",
    "\n",
    "ax[3].plot(val_recalls_tubb[::2], label=\"Recall\")\n",
    "ax[3].set_title(\"Recall\")\n",
    "ax[3].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num params in model\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"../training_output\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save metrics_tubb_all, metrics_non_tubb_all as json\n",
    "json.dump(metrics_tubb_all, open(f\"../training_output/val_metrics_tubb_all_overtones_{INSTRUMENT_OVERTONES}_normalized_{PATCH_NORMALIZE}_channels_{3}_targets_{NUM_TARGETS}_2.json\", \"w\"))\n",
    "json.dump(metrics_non_tubb_all, open(f\"../training_output/val_metrics_non_tubb_all_overtones_{INSTRUMENT_OVERTONES}_normalized_{PATCH_NORMALIZE}_channels_{3}_targets_{NUM_TARGETS}_2.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
